<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>3D Face Tracker</title>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.0" type="module"></script>
  <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.min.js"></script>
  <style>
    body, html {
      margin: 0;
      overflow: hidden;
      background: #000;
    }
    video, canvas {
      position: absolute;
      top: 0;
      left: 0;
      width: 100vw;
      height: 100vh;
      object-fit: cover;
    }
    #upload-container, #fps-display {
      position: absolute;
      top: 10px;
      left: 10px;
      background: rgba(255, 255, 255, 0.8);
      padding: 10px;
      border-radius: 8px;
      z-index: 10;
      font-family: monospace;
    }
    #fps-display {
      top: auto;
      bottom: 10px;
    }
  </style>
</head>
<body>
  <div id="upload-container">
    <label for="flameUpload">Upload FLAME model (.pkl): </label>
    <input type="file" id="flameUpload" accept=".pkl">
  </div>
  <div id="fps-display">FPS: ...</div>

  <video id="webcam" autoplay playsinline muted></video>
  <canvas id="overlay"></canvas>

  <script type="module">
    import { FaceDetector, FilesetResolver } from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.0";

    let faceDetector;
    let runningMode = "VIDEO";
    let ortSession = null;
    const fpsDisplay = document.getElementById('fps-display');
    let lastTimestamp = performance.now();
    const video = document.getElementById('webcam');
    const canvas = document.getElementById('overlay');
    const ctx = canvas.getContext('2d');

    let flameModelBuffer = null; // Store uploaded FLAME model here

    async function initBlazeFace() {
      const vision = await FilesetResolver.forVisionTasks(
        "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.0/wasm"
      );
      faceDetector = await FaceDetector.createFromOptions(vision, {
        baseOptions: {
          modelAssetPath: `https://storage.googleapis.com/mediapipe-models/face_detector/blaze_face_short_range/float16/1/blaze_face_short_range.tflite`,
          delegate: "GPU"
        },
        runningMode: runningMode
      });
    }

    function getCropBoxFromDetection(detection, width, height) {
      const scaleFactor = 1.6;
      const box = detection.boundingBox;
      const cx = box.originX + box.width / 2;
      const cy = box.originY + box.height / 2;
      const size = Math.max(box.width, box.height) * scaleFactor;
      const x = Math.max(0, cx - size / 2);
      const y = Math.max(0, cy - size / 2);
      const w = Math.min(width - x, size);
      const h = Math.min(height - y, size);
      return { x, y, width: w, height: h };
    }

    function getFaceCrop(box) {
      const tmpCanvas = document.createElement('canvas');
      tmpCanvas.width = 128;
      tmpCanvas.height = 128;
      const tmpCtx = tmpCanvas.getContext('2d');
      tmpCtx.drawImage(video, box.x, box.y, box.width, box.height, 0, 0, 128, 128);
      return tmpCanvas;
    }

    async function runLandmarkModel(cropCanvas) {
      const imageData = cropCanvas.getContext('2d').getImageData(0, 0, 128, 128);
      const input = new Float32Array(3 * 128 * 128);
      for (let i = 0; i < 128 * 128; ++i) {
        const pixel = i * 4;
        input[i] = imageData.data[pixel] / 255;
        input[128 * 128 + i] = imageData.data[pixel + 1] / 255;
        input[2 * 128 * 128 + i] = imageData.data[pixel + 2] / 255;
      }
      const tensor = new ort.Tensor('float32', input, [1, 3, 128, 128]);
      const results = await ortSession.run({ input: tensor });
      const flat = results.output.data;

      let landmarks = [];
      const numLandmarks = flat.length / 3;
      for (let i = 0; i < numLandmarks; ++i) {
        const x = flat[i];
        const y = flat[i + numLandmarks];
        const sigma = Math.exp(flat[i + 2 * numLandmarks]);
        landmarks.push({ x, y, sigma });
      }
      return landmarks;
    }

    function drawLandmarks(landmarks, cropBox, scaleX, scaleY) {
      ctx.fillStyle = 'cyan';
      const points = new Path2D();
      for (let i = 0; i < landmarks.length; i ++) {
        const pt = landmarks[i];
        const x = (pt.x / 2 + 0.5) * cropBox.width + cropBox.x;
        const y = (pt.y / 2 + 0.5) * cropBox.height + cropBox.y;
        points.moveTo(x * scaleX + 2, y * scaleY);
        points.arc(x * scaleX, y * scaleY, 2, 0, 2 * Math.PI);
      }
      ctx.fill(points);
    }

    async function renderLoop() {
      ctx.clearRect(0, 0, canvas.width, canvas.height);
      const detections = await faceDetector.detectForVideo(video, performance.now()).detections;
      const confidenceThreshold = 0.8;
      const validDetections = detections.filter(d => d.categories[0].score >= confidenceThreshold);

      if (validDetections.length > 0) {
        const box = getCropBoxFromDetection(validDetections[0], video.videoWidth, video.videoHeight);
        ctx.strokeStyle = 'lime';
        ctx.lineWidth = 2;
        ctx.strokeRect(box.x, box.y, box.width, box.height);

        const cropCanvas = getFaceCrop(box);
        const landmarks = await runLandmarkModel(cropCanvas);
        drawLandmarks(landmarks, box, 1, 1);
      }
      const now = performance.now();
      const fps = 1000 / (now - lastTimestamp);
      fpsDisplay.textContent = `FPS: ${fps.toFixed(1)}`;
      lastTimestamp = now;
      requestAnimationFrame(renderLoop);
    }

    async function setupCamera() {
      const stream = await navigator.mediaDevices.getUserMedia({ video: true });
      video.srcObject = stream;
      return new Promise(resolve => {
        video.onloadedmetadata = () => {
          canvas.width = video.videoWidth;
          canvas.height = video.videoHeight;
          resolve();
        };
      });
    }

    document.getElementById('flameUpload').addEventListener('change', (e) => {
      const file = e.target.files[0];
      if (file) {
        const reader = new FileReader();
        reader.onload = () => {
          flameModelBuffer = reader.result; // Store parsed buffer globally
          console.log("FLAME model loaded:", new Uint8Array(reader.result).slice(0, 20), '...');
        };
        reader.readAsArrayBuffer(file);
      }
    });

    window.onload = async () => {
      await initBlazeFace();
      ortSession = await ort.InferenceSession.create('mobilenet_landmarks.onnx');
      console.log("âœ… ONNX model loaded");
      await setupCamera();
      requestAnimationFrame(renderLoop);
    };
  </script>
</body>
</html>
